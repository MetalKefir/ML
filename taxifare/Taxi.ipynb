{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ML\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXIFARE_PATH = os.path.join(\"datasets\", \"NYCityTaxiFare\")\n",
    "FILE_NAME = \"train.csv\"\n",
    "SIZE_CHUNK = 10*(10**6)\n",
    "R = 6371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_index = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_index, :]\n",
    "    y_batch = y_train.values[rnd_index]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxifare_data(taxifare_path=TAXIFARE_PATH, file_name = FILE_NAME, size_chunk = SIZE_CHUNK):\n",
    "    csv_path = os.path.join(taxifare_path, file_name)\n",
    "    return pd.read_csv(csv_path, nrows = size_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(estimator, data_set, lables):\n",
    "    lables_pred = estimator.predict(data_set)\n",
    "    mse = mean_squared_error(lables, lables_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = R * c\n",
    "    \n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB = (-74.5, -71, 40, 42)\n",
    "nyc_map = plt.imread('https://aiblog.nl/download/nyc_-74.5_-72.8_40.5_41.8.png')\n",
    "\n",
    "BB_zoom = (-74.1, -73.7, 40.2, 42)\n",
    "nyc_map_zoom = plt.imread('https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/images/nyc_-74.1_-73.7_40.6_40.85.PNG?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_on_map(df, BB, nyc_map, s=10, alpha=0.2, color = False):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(18, 22))\n",
    "    axs[0].scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=alpha, c='r', s=s)\n",
    "    axs[0].set_xlim((BB[0], BB[1]))\n",
    "    axs[0].set_ylim((BB[2], BB[3]))\n",
    "    axs[0].set_title('Pickup locations')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].imshow(nyc_map, zorder=0, extent=BB)\n",
    "\n",
    "    axs[1].scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, alpha=alpha, c='b', s=s)\n",
    "    axs[1].set_xlim((BB[0], BB[1]))\n",
    "    axs[1].set_ylim((BB[2], BB[3]))\n",
    "    axs[1].set_title('Dropoff locations')\n",
    "    axs[1].axis('off')\n",
    "    axs[1].imshow(nyc_map, zorder=0, extent=BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_datapoints_from_water(df):\n",
    "    def lonlat_to_xy(longitude, latitude, dx, dy, BB):\n",
    "        return (dx*(longitude - BB[0])/(BB[1]-BB[0])).astype('int'), \\\n",
    "               (dy - dy*(latitude - BB[2])/(BB[3]-BB[2])).astype('int')\n",
    "\n",
    "    # define bounding box\n",
    "    BB = (-74.5, -71, 40.2, 42)\n",
    "    \n",
    "    # read nyc mask and turn into boolean map with\n",
    "    # land = True, water = False\n",
    "    nyc_mask = plt.imread('https://aiblog.nl/download/nyc_mask-74.5_-72.8_40.5_41.8.png')[:,:,0] > 0.9\n",
    "    \n",
    "    # calculate for each lon,lat coordinate the xy coordinate in the mask map\n",
    "    pickup_x, pickup_y = lonlat_to_xy(df.pickup_longitude, df.pickup_latitude, \n",
    "                                      nyc_mask.shape[1], nyc_mask.shape[0], BB)\n",
    "    dropoff_x, dropoff_y = lonlat_to_xy(df.dropoff_longitude, df.dropoff_latitude, \n",
    "                                      nyc_mask.shape[1], nyc_mask.shape[0], BB)    \n",
    "    # calculate boolean index\n",
    "    idx = nyc_mask[pickup_y, pickup_x] & nyc_mask[dropoff_y, dropoff_x]\n",
    "    \n",
    "    # return only datapoints on land\n",
    "    return df[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_distance(x1, x2, y1, y2, p):\n",
    "    return ((abs(x2 - x1) ** p) + (abs(y2 - y1)) ** p) ** (1 / p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxifare_load = load_taxifare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxifare = taxifare_load.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxifare = taxifare.loc[taxifare[\"passenger_count\"].between(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxifare = taxifare.loc[taxifare[\"fare_amount\"].between(2.5, 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxifare = taxifare.loc[taxifare[\"pickup_longitude\"].between(-74.3, -72)]\n",
    "taxifare = taxifare.loc[taxifare[\"dropoff_longitude\"].between(-74.3, -72)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxifare = taxifare.loc[taxifare[\"pickup_latitude\"].between(40.3, 42.5)]\n",
    "taxifare = taxifare.loc[taxifare[\"dropoff_latitude\"].between(40.3, 42.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old size: 9749181\n",
      "New size: 9749121\n"
     ]
    }
   ],
   "source": [
    "print('Old size: %d' % len(taxifare))\n",
    "taxifare = remove_datapoints_from_water(taxifare)\n",
    "print('New size: %d' % len(taxifare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxifare[\"distance_km\"] = haversine_np(taxifare['pickup_longitude'], taxifare['pickup_latitude'],\n",
    "                         taxifare['dropoff_longitude'], taxifare['dropoff_latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxifare = taxifare.loc[taxifare[\"distance_km\"].between(0.3, 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxifare[\"abs_lat_diff\"] = (taxifare[\"dropoff_latitude\"] - taxifare[\"pickup_latitude\"]).abs()\n",
    "taxifare[\"abs_lon_diff\"] = (taxifare[\"dropoff_longitude\"] - taxifare[\"pickup_longitude\"]).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxifare = taxifare.loc[taxifare[\"abs_lat_diff\"] > 0]\n",
    "taxifare = taxifare.loc[taxifare[\"abs_lon_diff\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxifare[\"manhattan\"] = minkowski_distance(taxifare[\"pickup_longitude\"], taxifare[\"dropoff_longitude\"],\n",
    "                                       taxifare[\"pickup_latitude\"], taxifare[\"dropoff_latitude\"], 1)\n",
    "\n",
    "taxifare[\"euclidean\"] = minkowski_distance(taxifare[\"pickup_longitude\"], taxifare[\"dropoff_longitude\"],\n",
    "                                       taxifare[\"pickup_latitude\"], taxifare[\"dropoff_latitude\"], 2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.kdeplot(taxifare[\"pickup_latitude\"])\n",
    "sns.kdeplot(taxifare[\"dropoff_latitude\"], color=\"red\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.kdeplot(taxifare[\"pickup_longitude\"])\n",
    "sns.kdeplot(taxifare[\"dropoff_longitude\"], color=\"red\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_on_map(taxifare, \n",
    "            BB_zoom, nyc_map_zoom, s=0.05, alpha=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.kdeplot(taxifare[\"distance_km\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.kdeplot(taxifare[\"fare_amount\"], color=\"red\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(taxifare[\"distance_km\"], taxifare[\"fare_amount\"], \"bo\", alpha = 0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corrs = taxifare.corr()\n",
    "\n",
    "plt.figure(figsize = (12, 12))\n",
    "sns.heatmap(corrs, annot = True, vmin = -1, vmax = 1, fmt = '.3f', cmap=plt.cm.PiYG_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"distance_km\"]\n",
    "\n",
    "#\"key\", \"pickup_datetime\", \"pickup_latitude\", \"dropoff_latitude\"\n",
    "#\"dropoff_longitude\", \"pickup_longitude\", \"abs_lat_diff\", \"abs_lon_diff\", \"euclidean\", \"manhattan\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = taxifare[columns]\n",
    "y = taxifare[\"fare_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(X, y, \"bo\", alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val= train_test_split(X, y, test_size = 20000, random_state = 42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(X_train, y_train, \"bo\", alpha = 0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(X_val, y_val, \"bo\", alpha = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL ZONE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gradientboost_reg = GradientBoostingRegressor(random_state=42, presort=False, n_estimators=20, max_depth=12)\n",
    "\n",
    "kneighbors_reg = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "          metric_params=None, n_jobs=-1, p=2)\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42, n_jobs=-1, criterion='mse')\n",
    "\n",
    "extratree_reg = ExtraTreesRegressor(random_state=42, n_jobs=-1, criterion='mse')\n",
    "\n",
    "nn_reg = MLPRegressor(learning_rate='adaptive', learning_rate_init=0.02, \n",
    "                      early_stopping = True, verbose= 5, random_state=42, \n",
    "                      hidden_layer_sizes=(300,), solver='sgd', momentum=0.7)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "estimators = [forest_reg, extratree_reg]\n",
    "\n",
    "for estimator in estimators:\n",
    "    print(\"\\nМодель\", estimator)\n",
    "    estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for estimator in estimators:\n",
    "    print(\"\\nМодель\", estimator)\n",
    "    print(\"RMSE train:\", rmse(estimator, X_train, y_train))\n",
    "    print(\"RMSE test:\", rmse(estimator, X_val, y_val))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "param_dist={\n",
    "            \"n_neighbors\": [80, 100, 120, 140],\n",
    "            \"weights\": [\"uniform\", \"distance\"]\n",
    "           }\n",
    "\n",
    "n_iter_search = 5\n",
    "random_search = RandomizedSearchCV(kneighbors_reg, param_dist, n_iter=n_iter_search, cv=5,verbose=5, n_jobs = -1,\n",
    "error_score = 'neg_mean_squared_error')\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"RMSE train:\", rmse(random_search.best_estimator_, X_train, y_train))\n",
    "print(\"RMSE test:\", rmse(random_search.best_estimator_, X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN MODEL ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = len(columns)\n",
    "n_hidden1 = 40\n",
    "n_hidden2 = 10\n",
    "#n_hidden3 = 150\n",
    "#n_hidden4 = 50\n",
    "#n_hidden5 = 20\n",
    "#n_hidden6 = 10\n",
    "n_outputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training = training, momentum = 0.9)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu, kernel_initializer = he_init)\n",
    "    bn1 = my_batch_norm_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\", activation=tf.nn.elu, kernel_initializer = he_init)\n",
    "    bn2 = my_batch_norm_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    \n",
    "#    hidden3 = tf.layers.dense(bn2_act, n_hidden3, name=\"hidden3\", activation=tf.nn.elu, kernel_initializer = he_init)\n",
    "#    bn3 = my_batch_norm_layer(hidden3)\n",
    "#    bn3_act = tf.nn.elu(bn3)\n",
    "    \n",
    "#    hidden4 = tf.layers.dense(bn3_act, n_hidden4, name=\"hidden4\", activation=tf.nn.relu, kernel_initializer = he_init)\n",
    " #   bn4 = my_batch_norm_layer(hidden4)\n",
    " #   bn4_act = tf.nn.elu(bn4)\n",
    "    \n",
    "#    hidden5 = tf.layers.dense(bn4_act, n_hidden5, name=\"hidden5\", activation=tf.nn.elu, kernel_initializer = he_init)\n",
    "#    bn5 = my_batch_norm_layer(hidden5)\n",
    "#    bn5_act = tf.nn.elu(bn5)\n",
    "    \n",
    "#    hidden6 = tf.layers.dense(bn5_act, n_hidden6, name=\"hidden6\", activation=tf.nn.elu, kernel_initializer = he_init)\n",
    "#    bn6 = my_batch_norm_layer(hidden6)\n",
    "#    bn6_act = tf.nn.elu(bn6)\n",
    "    \n",
    "    output = tf.layers.dense(bn2_act, n_outputs, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    mse = tf.losses.mean_squared_error(labels=y, predictions=output)\n",
    "    loss = tf.sqrt(mse, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    mse = tf.losses.mean_squared_error(labels=y, predictions=output)\n",
    "    rmse = tf.sqrt(mse, name=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Ошибка при обучении: 9.286522 Ошибка при проверке: 9.491141\n",
      "Найдена лучшая модель на  1  итерации с ошибкой при проверке: 9.491141\n",
      "1 Ошибка при обучении: 9.190817 Ошибка при проверке: 9.340859\n",
      "Найдена лучшая модель на  2  итерации с ошибкой при проверке: 9.340859\n",
      "2 Ошибка при обучении: 9.221015 Ошибка при проверке: 9.3438635\n",
      "3 Ошибка при обучении: 9.474977 Ошибка при проверке: 9.352812\n",
      "4 Ошибка при обучении: 9.296511 Ошибка при проверке: 9.340099\n",
      "Найдена лучшая модель на  5  итерации с ошибкой при проверке: 9.340099\n",
      "5 Ошибка при обучении: 9.411121 Ошибка при проверке: 9.341508\n",
      "6 Ошибка при обучении: 9.239766 Ошибка при проверке: 9.340295\n",
      "7 Ошибка при обучении: 9.307523 Ошибка при проверке: 9.340271\n",
      "8 Ошибка при обучении: 9.32244 Ошибка при проверке: 9.3403015\n",
      "9 Ошибка при обучении: 9.370398 Ошибка при проверке: 9.351635\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 10000\n",
    "n_batches = int(np.ceil(X_train.shape[0]/ batch_size))\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "error = float(\"inf\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        rmse_train = rmse.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        rmse_test = rmse.eval(feed_dict={X: X_val, y: y_val})\n",
    "        \n",
    "        print(epoch, \"Ошибка при обучении:\", rmse_train,\n",
    "                     \"Ошибка при проверке:\", rmse_test)\n",
    "        \n",
    "        if rmse_test < error:\n",
    "            error = rmse_test\n",
    "            save_path = saver.save(sess, \"./taxifare/best_model_final2/best_model_final2.ckpt\")\n",
    "            print(\"Найдена лучшая модель на \", epoch+1, \" итерации с ошибкой при проверке:\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TAXIFARE_PATH = os.path.join(\"datasets\", \"NYCityTaxiFare\")\n",
    "FILE_NAME = \"test.csv\"\n",
    "csv_path = os.path.join(TAXIFARE_PATH, FILE_NAME)\n",
    "\n",
    "X_test_final = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test[\"abs_lat_diff\"] = (X_test[\"dropoff_latitude\"] - X_test[\"pickup_latitude\"]).abs()\n",
    "#X_test[\"abs_lon_diff\"] = (X_test[\"dropoff_longitude\"] - X_test[\"pickup_longitude\"]).abs()\n",
    "\n",
    "#X_test = X_test.loc[X_test[\"abs_lat_diff\"] > 0]\n",
    "#X_test = X_test.loc[X_test[\"abs_lon_diff\"] > 0]\n",
    "\n",
    "X_test[\"distance_km\"] = haversine_np(X_test['pickup_longitude'], X_test['pickup_latitude'],\n",
    "                         X_test['dropoff_longitude'], X_test['dropoff_latitude'])\n",
    "\n",
    "X_test[\"manhattan\"] = minkowski_distance(X_test[\"pickup_longitude\"], X_test[\"dropoff_longitude\"],\n",
    "                                       X_test[\"pickup_latitude\"], X_test[\"dropoff_latitude\"], 1)\n",
    "\n",
    "X_test[\"euclidean\"] = minkowski_distance(X_test[\"pickup_longitude\"], X_test[\"dropoff_longitude\"],\n",
    "                                       X_test[\"pickup_latitude\"], X_test[\"dropoff_latitude\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[columns]\n",
    "X_test = pipeline.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./taxifare/best_model_final2/best_model_final2.ckpt\")\n",
    "    y_pred = output.eval(feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_pred, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXIFARE_PATH = os.path.join(\"datasets\", \"NYCityTaxiFare\")\n",
    "csv_path = os.path.join(TAXIFARE_PATH, 'submission2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(\n",
    "    {'key': X_test_final.key, 'fare_amount': y_pred},\n",
    "    columns = ['key', 'fare_amount'])\n",
    "submission.to_csv(csv_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
